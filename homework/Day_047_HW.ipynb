{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "了解如何使用 Sklearn 中的 hyper-parameter search 找出最佳的超參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業\n",
    "請使用不同的資料集，並使用 hyper-parameter search 的方式，看能不能找出最佳的超參數組合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring the dataset\n",
    "You start off by collecting the dataset. It can be found both online and in our GitHub repository, so you can also just fetch it with wget (note: make sure you first type pip install wget into your terminal since wget is not a preinstalled Python library). This command will download a copy of the dataset to your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                              ]     0 / 84199\r",
      "  9% [.......                                                                       ]  8192 / 84199\r",
      " 19% [...............                                                               ] 16384 / 84199\r",
      " 29% [......................                                                        ] 24576 / 84199\r",
      " 38% [..............................                                                ] 32768 / 84199\r",
      " 48% [.....................................                                         ] 40960 / 84199\r",
      " 58% [.............................................                                 ] 49152 / 84199\r",
      " 68% [.....................................................                         ] 57344 / 84199\r",
      " 77% [............................................................                  ] 65536 / 84199\r",
      " 87% [....................................................................          ] 73728 / 84199\r",
      " 97% [...........................................................................   ] 81920 / 84199\r",
      "100% [..............................................................................] 84199 / 84199"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "# Import the dataset\n",
    "data_url = 'https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/datasets/wine/winequality-red.csv'\n",
    "dataset = wget.download(data_url)\n",
    "dataset = pd.read_csv(dataset, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a brief look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides\n",
       "0            7.4              0.70         0.00             1.9      0.076\n",
       "1            7.8              0.88         0.00             2.6      0.098\n",
       "2            7.8              0.76         0.04             2.3      0.092\n",
       "3           11.2              0.28         0.56             1.9      0.075\n",
       "4            7.4              0.70         0.00             1.9      0.076"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peak at the first few columns of the data\n",
    "first_5_columns = dataset.columns[0:5]\n",
    "dataset[first_5_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the dimensions of the dataset and the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n",
      "['fixed acidity' 'volatile acidity' 'citric acid' 'residual sugar'\n",
      " 'chlorides' 'free sulfur dioxide' 'total sulfur dioxide' 'density' 'pH'\n",
      " 'sulphates' 'alcohol' 'quality']\n"
     ]
    }
   ],
   "source": [
    "# Eximine shape of dataset and the column names:\n",
    "print(dataset.shape)\n",
    "print(dataset.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like you have a dozen features to play with, and just under 1600 data points. Get some summary statistics on the features using describe():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  \n",
       "count  1599.000000  \n",
       "mean      0.087467  \n",
       "std       0.047065  \n",
       "min       0.012000  \n",
       "25%       0.070000  \n",
       "50%       0.079000  \n",
       "75%       0.090000  \n",
       "max       0.611000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarise feature values\n",
    "dataset.describe()[first_5_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    681\n",
       "6    638\n",
       "7    199\n",
       "4     53\n",
       "8     18\n",
       "3     10\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the outcome quality is a bit funcky - the values are mostly 5 and 6. This could get a bit irksome later on, so go ahead and recode the quality scores into something more convenient. One idea would be to label wines as being either high quility (e.g. if their score is 6 or higher) or low quility (if the score is 5 or lower). You could encode this with a 1 representing high quility and 0 representing low quility, like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a lambda function to bin quility scores\n",
    "dataset['quality_is_high'] = dataset.quality.apply(lambda x: 1 if x>=6 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the pandas dataframe into a numpy array and isolate the outcome variable you’d like to predict (‘quality_is_high’). This conversion is needed to feed the data into the machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the dataframe to a numpy array and split the\n",
    "# data into an input matrix X and class label vector y\n",
    "npArray = np.array(dataset)\n",
    "X = npArray[:,:-2].astype(float)\n",
    "y = npArray[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the dataset set up, the machine learning can begin. First, you have to split the dataset into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X,y,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Random Forest\n",
    "Briefly, random forests build a collection of classification trees, where each tree tries to classify data points into classes by recursively splitting the data on the features (and feature values) that seperate the classes best. Each tree is trained on bootstrapped data, and ach bifurcation point is only allowed to 'see' a subset of the availalble variables when deciding on the best split. So, these two elements of randomness are introduced when constructing each tree, which means that a variety of different trees are built. The random forest then ensembles these base learners together, i.e. it combines these trees into an aggregated model. In the end, when you want to classify a new data point, the individual trees each make their individual predictions and the random forest surveys these opinions and accepts the majority position. This approach often leads to improved accuracy, generalizability, and stability in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting wine quality with a random forest\n",
    "Out of the box, scikit's random forest classifier performs reasonbly well on the wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.85      0.81       188\n",
      "         1.0       0.85      0.79      0.82       212\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.82      0.82      0.81       400\n",
      "weighted avg       0.82      0.81      0.82       400\n",
      "\n",
      "Overall Accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(XTrain,yTrain)\n",
    "\n",
    "rf_predictions = rf.predict(XTest)\n",
    "\n",
    "print(metrics.classification_report(yTest,rf_predictions))\n",
    "print(\"Overall Accuracy:\", round(metrics.accuracy_score(yTest,rf_predictions),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has an overall accuracy around the 0.81 mark (there is some variability in this value – try rerunning the code block several times, or setting different seeds using random_state). This means that 81% of the time, your model is able to predict the right class when given the test data. In other words, it can distinguish pretty well between a good and a bad wine based on their chemical properties.\n",
    "\n",
    "Next up, you are going to learn how to pick the best values for the hyperparameters of the random forest algorithm in order to get better models with (hopefully!) even higher accuracy than this baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better modelling through hyperparameter optimization\n",
    "We’ve glossed over what a hyperparameter actually is. Let’s explore the topic now. Often, when setting out to train a machine learning algorithm on your dataset of interest, you must first specify a number of arguments or hyperparameters (HPs). An HP is just a variable than influences the performance of your model, but isn’t directly tuned during model training.\n",
    "\n",
    "For example, when using the random forest algorithm to do classification, you have to set the value of the hyperparameter n_estimators ahead of time, before training commences. n_estimators controls the number of individual trees in the random forest ensemble. The more the better (with diminishing returns), but more trees come at the expense of longer training time.\n",
    "\n",
    "As mentioned above, scikit-learn generally provides reasonable hyperparameter default values, such that it is possible to quickly build a random forest classifier by simply typing RandomForestClassifier() and then fitting it to your data. You can get the documentation on what hyperparameter values the classifier has automatically assumed, but you can also examine model details directly using get_params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)>\n"
     ]
    }
   ],
   "source": [
    "# Create a default random forest classifier and print its parameter\n",
    "rf_default = RandomForestClassifier()\n",
    "print(rf_default.get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that n_estimators takes on a default value of 100. Other hyperparameters include max_features, which controls the size of the random selection of features the algorithm is allowed to consider when splitting a node. The default is max_features='auto', where the auto refers to sqrt of the number of all features in classification problems. So for instance, if you have 16 features in total, then trees are restricted to considering only 4 features at each bifurcation point (instead of searching all features for the best split). Other important HPs include the max_depth, which restricts the depth of the trees you grow, and criterion, which dictates how trees calculate the class purity resulting from splits.\n",
    "\n",
    "As you saw above, the default settings for random forests do a good job. But it’s a good idea to try to improve your learning algorithm’s performance. But how do you know what values to set the hyperparameters to in order to get the best performance from your learning algorithms?\n",
    "\n",
    "You optimize hyperparameters in exactly the way that you might expect – you try different values and see what works best. However, some care is needed when deciding how exactly to measure if certain values work well, and which strategy to use to systematically explore hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning your random forest\n",
    "In order to build the best possible model that does a good job at describing the underlying trends in a dataset, you need to pick the right HP values. The most basic strategy to do this would be just to test different possible values for the HPs and see how the model performs.\n",
    "\n",
    "Let’s try out some random HP values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually specify some HP values\n",
    "hp_combinations= [\n",
    "    {\"n_estimators\": 100, \"max_features\": None},\n",
    "    {\"n_estimators\": 140, \"max_features\": 'log2',},\n",
    "    {\"n_estimators\": 180, \"max_features\": 'sqrt'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manually write a small loop to test out how well the different combinations of these potential HP values fare (later, you’ll see better ways to do this):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When n_estimators is 100 and max_features is None, test set accuracy is 0.82\n",
      "When n_estimators is 140 and max_features is log2, test set accuracy is 0.82\n",
      "When n_estimators is 180 and max_features is sqrt, test set accuracy is 0.82\n"
     ]
    }
   ],
   "source": [
    "# test out different HP combinations\n",
    "for hp_combn in hp_combinations:\n",
    "    \n",
    "    # Train and output accuracies\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=hp_combn[\"n_estimators\"],\n",
    "        max_features = hp_combn[\"max_features\"]\n",
    "    )\n",
    "    \n",
    "    rf.fit(XTrain,yTrain)\n",
    "    rf_predictions = rf.predict(XTest)\n",
    "    print(\"When n_estimators is {} and max_features is {}, test set accuracy is {}\".\n",
    "          format(hp_combn[\"n_estimators\"],hp_combn[\"max_features\"],round(metrics.accuracy_score(yTest,rf_predictions),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the last combinations of HPs might be doing better. However, manually searching for the best HPs in this way is not efficient, it’s a bit random, and is liable to miss good combinations. There is, however, a solution – grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "Traditionally and perhaps most intuitively, scanning for good HPs values can be done with grid search (also called parameter sweep). This strategy exhaustively searches through some manually prespecified HP values and reports the best option. It is common to try to optimize multiple HPs simultaneously – grid search tries each combination of HPs in turn and reports the best one, hence the name ‘grid’. This is a more convenient and complete way of searching through hyperparameter space than manually specifying combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, None), (100, 'log2'), (100, 'sqrt'), (140, None), (140, 'log2'), (140, 'sqrt'), (180, None), (180, 'log2'), (180, 'sqrt')]\n",
      "The number of HP combinations is : 9\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "n_estimators = [100, 140, 180]\n",
    "max_features = [None, 'log2', 'sqrt']\n",
    "\n",
    "hp_combinations = list(itertools.product(n_estimators,max_features))\n",
    "print(hp_combinations)\n",
    "print(\"The number of HP combinations is : {}\".format(len(hp_combinations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However there is a massive pitfall here! Scanning through all possible combinations of HPs to build models and evaluating them on the test set will output the combination of parameters that does best, but these values might not generalise well. This approach is less misguided than trying to optimize models by evaluating them on the training set, but is still not ideal. The problem is that during repeated evaluation on the test dataset, knowledge of the test set can leak into the model building phase. You are at risk of inadvertently learning something about the test set, and hence are susceptible to overfitting. How does one get around these issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search with k-fold cross validation for hyperparameter tuning\n",
    "Enter k-fold cross-validation, which is a handy technique for measuring a model’s performance using only the training set. k-fold CV is a general method (see an explanation here), and is not specific to hyperparameter optimization, but is very useful for that purpose. You simply try out different HP values, get several different estimates of model performance for each HP value (or combination of HP values), and choose the model with the lowest CV error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of HP optimization, you perform k-fold cross validation together with grid search to get a more robust estimate of the model performance associated with specific HP values. The combination of grid search and k-fold cross validation is very popular for finding the models with good performance and generalisability. So, in HP optimisation, you are actually trying to do two things:\n",
    "* Find the combination of HPs that improves model performance (e.g. accuracy)\n",
    "* Make sure that this choice of HPs will generalize well to new data\n",
    "\n",
    "The CV is there to address the second concern. scikit-learn makes grid search with k-fold CV very easy and slick to do, and even supports parallel distributing of the search (via the n_jobs argument). The set-up looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "\n",
    "n_estimators = list(np.arange(100,600,20))\n",
    "max_features = [None,'sqrt','log2']\n",
    "\n",
    "hyperparameters = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features\n",
    "}\n",
    "\n",
    "# Grid search using cross-validation\n",
    "gridCV = GridSearchCV(RandomForestClassifier(),param_grid = hyperparameters,cv=5,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_features': [None, 'sqrt', 'log2'],\n",
       "                         'n_estimators': [100, 120, 140, 160, 180, 200, 220,\n",
       "                                          240, 260, 280, 300, 320, 340, 360,\n",
       "                                          380, 400, 420, 440, 460, 480, 500,\n",
       "                                          520, 540, 560, 580]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform grid search with 5 fold\n",
    "gridCV.fit(XTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing n_estimators value is: 280\n",
      "The best performing max_features value is: log2\n"
     ]
    }
   ],
   "source": [
    "# Identify grid search hyperparameters values\n",
    "best_n_estim = gridCV.best_params_['n_estimators']\n",
    "best_max_features = gridCV.best_params_['max_features']\n",
    "\n",
    "print(\"The best performing n_estimators value is: {}\".format(best_n_estim))\n",
    "print(\"The best performing max_features value is: {}\".format(best_max_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visually represent the results from this grid search (stored in gridCV.grid_scores_) with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.56900377, 0.68570466, 0.84397712, 0.99118853, 1.10858188,\n",
       "        1.19804635, 1.3474524 , 1.50234489, 1.56487975, 1.742313  ,\n",
       "        1.97470341, 1.98776789, 2.11473546, 2.24030704, 2.38033705,\n",
       "        2.84305058, 3.00732861, 3.05886378, 2.95881758, 3.17395473,\n",
       "        3.15568352, 3.2469481 , 3.50478725, 3.54188361, 3.66156602,\n",
       "        0.33960667, 0.40543327, 0.47305517, 0.55533905, 0.63762522,\n",
       "        0.70225687, 0.78752866, 0.84738388, 0.89316096, 1.01373687,\n",
       "        1.06510253, 1.17132907, 1.22279191, 1.29639878, 1.32970886,\n",
       "        1.41578627, 1.53347316, 1.56549244, 1.61665912, 1.67859092,\n",
       "        1.77244554, 1.86191616, 1.90750318, 1.98112793, 2.05650907,\n",
       "        0.35276942, 0.45341053, 0.52063336, 0.56162214, 0.66465144,\n",
       "        0.71023092, 0.78623734, 0.85615568, 0.95808401, 0.99269609,\n",
       "        1.10809522, 1.15288625, 1.21451106, 1.30257792, 1.37568879,\n",
       "        1.45398293, 1.50126429, 1.59512043, 1.66044502, 1.76139455,\n",
       "        1.78401918, 1.85613241, 1.96474619, 2.04592814, 1.77633872]),\n",
       " 'std_fit_time': array([0.01121283, 0.01829791, 0.02854112, 0.02508646, 0.0360595 ,\n",
       "        0.02145986, 0.03704943, 0.05715628, 0.04052136, 0.04747037,\n",
       "        0.10875455, 0.06974595, 0.07114881, 0.08589526, 0.09128715,\n",
       "        0.2291666 , 0.2668322 , 0.16625746, 0.19727524, 0.10828842,\n",
       "        0.10192672, 0.13523966, 0.16893294, 0.152708  , 0.10853834,\n",
       "        0.01526347, 0.00996269, 0.01482629, 0.01624757, 0.02849307,\n",
       "        0.02946643, 0.03224848, 0.03029933, 0.02225493, 0.03105529,\n",
       "        0.04056719, 0.05918832, 0.0638741 , 0.04007901, 0.04912769,\n",
       "        0.05552901, 0.06126391, 0.04930909, 0.05374369, 0.04168124,\n",
       "        0.06349982, 0.08956264, 0.08222077, 0.05560475, 0.08532571,\n",
       "        0.01240176, 0.01963571, 0.04440969, 0.01318788, 0.04135072,\n",
       "        0.03682436, 0.02312866, 0.04734541, 0.05344181, 0.0381279 ,\n",
       "        0.05988687, 0.04640342, 0.04358238, 0.06379914, 0.07120324,\n",
       "        0.07377408, 0.06404284, 0.04892978, 0.1059363 , 0.08994214,\n",
       "        0.08467778, 0.07377136, 0.07974545, 0.07557252, 0.1735756 ]),\n",
       " 'mean_score_time': array([0.01506238, 0.01834955, 0.02363663, 0.02463584, 0.02623162,\n",
       "        0.0328135 , 0.03421011, 0.0374042 , 0.03910065, 0.0455832 ,\n",
       "        0.05056987, 0.0473753 , 0.05326076, 0.05415978, 0.05904188,\n",
       "        0.07769442, 0.08407717, 0.0703166 , 0.07271166, 0.08018918,\n",
       "        0.07679429, 0.08487506, 0.08946362, 0.08916459, 0.09584923,\n",
       "        0.01535897, 0.0187499 , 0.02224255, 0.02563462, 0.03062015,\n",
       "        0.03311048, 0.0373045 , 0.03869672, 0.04328475, 0.04886956,\n",
       "        0.04997163, 0.05206294, 0.05415468, 0.05844755, 0.06553102,\n",
       "        0.06403332, 0.06772285, 0.07261229, 0.07340822, 0.07709594,\n",
       "        0.0839788 , 0.08357654, 0.08647609, 0.0931572 , 0.10382829,\n",
       "        0.01635981, 0.02503524, 0.02344098, 0.02593803, 0.03042021,\n",
       "        0.03401122, 0.03769908, 0.03999562, 0.04318657, 0.04398317,\n",
       "        0.0537601 , 0.05365806, 0.05635104, 0.05874658, 0.06203856,\n",
       "        0.06532741, 0.0718122 , 0.07500405, 0.0784997 , 0.08278022,\n",
       "        0.08327909, 0.09065957, 0.0856729 , 0.09405818, 0.05557337]),\n",
       " 'std_score_time': array([0.00101582, 0.00135468, 0.003699  , 0.00132203, 0.00097726,\n",
       "        0.00309551, 0.00188413, 0.0025597 , 0.00174487, 0.00313161,\n",
       "        0.00337838, 0.00118329, 0.00341569, 0.0010259 , 0.00270539,\n",
       "        0.02252309, 0.02664086, 0.00384875, 0.00273656, 0.00507843,\n",
       "        0.00427774, 0.00631462, 0.01486008, 0.00633641, 0.00622534,\n",
       "        0.00079796, 0.00074628, 0.00039943, 0.00152952, 0.00171555,\n",
       "        0.00116446, 0.00096765, 0.00171573, 0.00106041, 0.00595064,\n",
       "        0.00400667, 0.00203518, 0.00087366, 0.0018267 , 0.00810908,\n",
       "        0.00120728, 0.00257094, 0.00547021, 0.00254844, 0.00298551,\n",
       "        0.00790892, 0.00429692, 0.00432791, 0.00468908, 0.02222002,\n",
       "        0.00073342, 0.00579998, 0.00077135, 0.00063373, 0.00425806,\n",
       "        0.00228659, 0.00263116, 0.00155566, 0.00251616, 0.00079688,\n",
       "        0.00923801, 0.00443203, 0.00383568, 0.00306334, 0.00228983,\n",
       "        0.00189537, 0.00826421, 0.00743794, 0.00598944, 0.00998824,\n",
       "        0.00682224, 0.0113185 , 0.00161872, 0.00783732, 0.0091588 ]),\n",
       " 'param_max_features': masked_array(data=[None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None, None, None, None, None, 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300,\n",
       "                    320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520,\n",
       "                    540, 560, 580, 100, 120, 140, 160, 180, 200, 220, 240,\n",
       "                    260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460,\n",
       "                    480, 500, 520, 540, 560, 580, 100, 120, 140, 160, 180,\n",
       "                    200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400,\n",
       "                    420, 440, 460, 480, 500, 520, 540, 560, 580],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_features': None, 'n_estimators': 100},\n",
       "  {'max_features': None, 'n_estimators': 120},\n",
       "  {'max_features': None, 'n_estimators': 140},\n",
       "  {'max_features': None, 'n_estimators': 160},\n",
       "  {'max_features': None, 'n_estimators': 180},\n",
       "  {'max_features': None, 'n_estimators': 200},\n",
       "  {'max_features': None, 'n_estimators': 220},\n",
       "  {'max_features': None, 'n_estimators': 240},\n",
       "  {'max_features': None, 'n_estimators': 260},\n",
       "  {'max_features': None, 'n_estimators': 280},\n",
       "  {'max_features': None, 'n_estimators': 300},\n",
       "  {'max_features': None, 'n_estimators': 320},\n",
       "  {'max_features': None, 'n_estimators': 340},\n",
       "  {'max_features': None, 'n_estimators': 360},\n",
       "  {'max_features': None, 'n_estimators': 380},\n",
       "  {'max_features': None, 'n_estimators': 400},\n",
       "  {'max_features': None, 'n_estimators': 420},\n",
       "  {'max_features': None, 'n_estimators': 440},\n",
       "  {'max_features': None, 'n_estimators': 460},\n",
       "  {'max_features': None, 'n_estimators': 480},\n",
       "  {'max_features': None, 'n_estimators': 500},\n",
       "  {'max_features': None, 'n_estimators': 520},\n",
       "  {'max_features': None, 'n_estimators': 540},\n",
       "  {'max_features': None, 'n_estimators': 560},\n",
       "  {'max_features': None, 'n_estimators': 580},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 100},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 120},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 140},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 160},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 180},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 200},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 220},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 240},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 260},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 280},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 300},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 320},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 340},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 360},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 380},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 400},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 420},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 440},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 460},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 480},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 500},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 520},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 540},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 560},\n",
       "  {'max_features': 'sqrt', 'n_estimators': 580},\n",
       "  {'max_features': 'log2', 'n_estimators': 100},\n",
       "  {'max_features': 'log2', 'n_estimators': 120},\n",
       "  {'max_features': 'log2', 'n_estimators': 140},\n",
       "  {'max_features': 'log2', 'n_estimators': 160},\n",
       "  {'max_features': 'log2', 'n_estimators': 180},\n",
       "  {'max_features': 'log2', 'n_estimators': 200},\n",
       "  {'max_features': 'log2', 'n_estimators': 220},\n",
       "  {'max_features': 'log2', 'n_estimators': 240},\n",
       "  {'max_features': 'log2', 'n_estimators': 260},\n",
       "  {'max_features': 'log2', 'n_estimators': 280},\n",
       "  {'max_features': 'log2', 'n_estimators': 300},\n",
       "  {'max_features': 'log2', 'n_estimators': 320},\n",
       "  {'max_features': 'log2', 'n_estimators': 340},\n",
       "  {'max_features': 'log2', 'n_estimators': 360},\n",
       "  {'max_features': 'log2', 'n_estimators': 380},\n",
       "  {'max_features': 'log2', 'n_estimators': 400},\n",
       "  {'max_features': 'log2', 'n_estimators': 420},\n",
       "  {'max_features': 'log2', 'n_estimators': 440},\n",
       "  {'max_features': 'log2', 'n_estimators': 460},\n",
       "  {'max_features': 'log2', 'n_estimators': 480},\n",
       "  {'max_features': 'log2', 'n_estimators': 500},\n",
       "  {'max_features': 'log2', 'n_estimators': 520},\n",
       "  {'max_features': 'log2', 'n_estimators': 540},\n",
       "  {'max_features': 'log2', 'n_estimators': 560},\n",
       "  {'max_features': 'log2', 'n_estimators': 580}],\n",
       " 'split0_test_score': array([0.78333333, 0.77916667, 0.80416667, 0.77916667, 0.79166667,\n",
       "        0.79583333, 0.79166667, 0.79166667, 0.7875    , 0.7875    ,\n",
       "        0.8       , 0.79583333, 0.78333333, 0.7875    , 0.78333333,\n",
       "        0.78333333, 0.8       , 0.7875    , 0.78333333, 0.78333333,\n",
       "        0.79166667, 0.79583333, 0.7875    , 0.8       , 0.79166667,\n",
       "        0.8       , 0.8       , 0.7875    , 0.8       , 0.79166667,\n",
       "        0.7875    , 0.8       , 0.80416667, 0.80416667, 0.79166667,\n",
       "        0.80416667, 0.78333333, 0.79166667, 0.7875    , 0.78333333,\n",
       "        0.79166667, 0.7875    , 0.79583333, 0.7875    , 0.7875    ,\n",
       "        0.7875    , 0.7875    , 0.7875    , 0.79166667, 0.79583333,\n",
       "        0.79166667, 0.78333333, 0.77916667, 0.8       , 0.80833333,\n",
       "        0.7875    , 0.79583333, 0.79583333, 0.7875    , 0.80833333,\n",
       "        0.7875    , 0.7875    , 0.79166667, 0.80833333, 0.79583333,\n",
       "        0.7875    , 0.79583333, 0.79166667, 0.7875    , 0.79583333,\n",
       "        0.79583333, 0.78333333, 0.8       , 0.7875    , 0.8       ]),\n",
       " 'split1_test_score': array([0.73333333, 0.74166667, 0.75      , 0.74166667, 0.74583333,\n",
       "        0.74583333, 0.725     , 0.73333333, 0.725     , 0.74583333,\n",
       "        0.73333333, 0.7375    , 0.73333333, 0.7375    , 0.75      ,\n",
       "        0.74166667, 0.74166667, 0.75416667, 0.74166667, 0.72916667,\n",
       "        0.72916667, 0.7375    , 0.73333333, 0.74166667, 0.7375    ,\n",
       "        0.74583333, 0.73333333, 0.7375    , 0.73333333, 0.73333333,\n",
       "        0.74583333, 0.74166667, 0.73333333, 0.72916667, 0.7375    ,\n",
       "        0.7375    , 0.75416667, 0.72916667, 0.73333333, 0.73333333,\n",
       "        0.7375    , 0.72083333, 0.74583333, 0.7375    , 0.73333333,\n",
       "        0.72916667, 0.74166667, 0.73333333, 0.74166667, 0.73333333,\n",
       "        0.72916667, 0.74583333, 0.7375    , 0.73333333, 0.7375    ,\n",
       "        0.7375    , 0.74166667, 0.74166667, 0.74166667, 0.7375    ,\n",
       "        0.7375    , 0.74583333, 0.7375    , 0.73333333, 0.74166667,\n",
       "        0.74583333, 0.7375    , 0.73333333, 0.7375    , 0.73333333,\n",
       "        0.7375    , 0.7375    , 0.7375    , 0.73333333, 0.74166667]),\n",
       " 'split2_test_score': array([0.82083333, 0.82083333, 0.83333333, 0.8375    , 0.80833333,\n",
       "        0.81666667, 0.80416667, 0.8125    , 0.81666667, 0.80416667,\n",
       "        0.82083333, 0.82916667, 0.81666667, 0.8125    , 0.82083333,\n",
       "        0.80416667, 0.8125    , 0.82916667, 0.83333333, 0.8125    ,\n",
       "        0.82916667, 0.82916667, 0.82083333, 0.80833333, 0.81666667,\n",
       "        0.83333333, 0.8375    , 0.84583333, 0.83333333, 0.83333333,\n",
       "        0.82916667, 0.8375    , 0.84583333, 0.83333333, 0.85      ,\n",
       "        0.8375    , 0.82916667, 0.85416667, 0.84583333, 0.84583333,\n",
       "        0.8375    , 0.84583333, 0.85      , 0.83333333, 0.84166667,\n",
       "        0.84166667, 0.84166667, 0.83333333, 0.84166667, 0.83333333,\n",
       "        0.83333333, 0.825     , 0.84166667, 0.82916667, 0.83333333,\n",
       "        0.83333333, 0.84583333, 0.84166667, 0.83333333, 0.84583333,\n",
       "        0.84166667, 0.85      , 0.85416667, 0.84583333, 0.8375    ,\n",
       "        0.82916667, 0.84166667, 0.83333333, 0.82916667, 0.84166667,\n",
       "        0.83333333, 0.83333333, 0.83333333, 0.8375    , 0.83333333]),\n",
       " 'split3_test_score': array([0.79583333, 0.79166667, 0.77916667, 0.79583333, 0.7875    ,\n",
       "        0.7875    , 0.79166667, 0.7875    , 0.7875    , 0.79583333,\n",
       "        0.79166667, 0.79166667, 0.78333333, 0.7875    , 0.80833333,\n",
       "        0.79166667, 0.78333333, 0.79166667, 0.80416667, 0.79166667,\n",
       "        0.7875    , 0.79583333, 0.79583333, 0.79583333, 0.79583333,\n",
       "        0.8125    , 0.80833333, 0.81666667, 0.8       , 0.80416667,\n",
       "        0.8125    , 0.8125    , 0.80416667, 0.8125    , 0.82083333,\n",
       "        0.80416667, 0.8       , 0.82083333, 0.8125    , 0.81666667,\n",
       "        0.8125    , 0.8       , 0.80416667, 0.80833333, 0.81666667,\n",
       "        0.82083333, 0.81666667, 0.81666667, 0.8125    , 0.80833333,\n",
       "        0.79166667, 0.79166667, 0.79583333, 0.81666667, 0.8125    ,\n",
       "        0.80833333, 0.82083333, 0.8125    , 0.82083333, 0.82083333,\n",
       "        0.80416667, 0.80833333, 0.8125    , 0.8125    , 0.82083333,\n",
       "        0.80833333, 0.80416667, 0.80416667, 0.8125    , 0.81666667,\n",
       "        0.80833333, 0.8125    , 0.8125    , 0.8125    , 0.80416667]),\n",
       " 'split4_test_score': array([0.79079498, 0.79079498, 0.77824268, 0.78661088, 0.79916318,\n",
       "        0.79916318, 0.79497908, 0.78661088, 0.79497908, 0.78242678,\n",
       "        0.79497908, 0.79916318, 0.79497908, 0.79497908, 0.79079498,\n",
       "        0.79916318, 0.79079498, 0.79497908, 0.79497908, 0.78661088,\n",
       "        0.80334728, 0.79497908, 0.79497908, 0.79079498, 0.79079498,\n",
       "        0.77824268, 0.76987448, 0.75732218, 0.77824268, 0.77405858,\n",
       "        0.77824268, 0.76987448, 0.77824268, 0.77405858, 0.77824268,\n",
       "        0.79079498, 0.76569038, 0.77824268, 0.79079498, 0.77824268,\n",
       "        0.77824268, 0.78242678, 0.77824268, 0.79079498, 0.79079498,\n",
       "        0.77824268, 0.76987448, 0.77405858, 0.79079498, 0.78661088,\n",
       "        0.76569038, 0.78661088, 0.79497908, 0.76987448, 0.77405858,\n",
       "        0.80334728, 0.78242678, 0.76987448, 0.78661088, 0.78242678,\n",
       "        0.79079498, 0.79916318, 0.77405858, 0.77405858, 0.78242678,\n",
       "        0.78242678, 0.77405858, 0.78661088, 0.77824268, 0.77405858,\n",
       "        0.77405858, 0.77405858, 0.78242678, 0.78661088, 0.79497908]),\n",
       " 'mean_test_score': array([0.78482566, 0.78482566, 0.78898187, 0.78815551, 0.7864993 ,\n",
       "        0.7889993 , 0.78149582, 0.78232218, 0.78232915, 0.78315202,\n",
       "        0.78816248, 0.79066597, 0.78232915, 0.78399582, 0.790659  ,\n",
       "        0.7839993 , 0.785659  , 0.79149582, 0.79149582, 0.78065551,\n",
       "        0.78816946, 0.79066248, 0.78649582, 0.78732566, 0.78649233,\n",
       "        0.79398187, 0.78980823, 0.78896444, 0.78898187, 0.78731172,\n",
       "        0.79064854, 0.79230823, 0.79314854, 0.79064505, 0.79564854,\n",
       "        0.79482566, 0.78647141, 0.7948152 , 0.79399233, 0.79148187,\n",
       "        0.79148187, 0.78731869, 0.7948152 , 0.79149233, 0.79399233,\n",
       "        0.79148187, 0.7914749 , 0.78897838, 0.795659  , 0.79148884,\n",
       "        0.78230474, 0.78648884, 0.78982915, 0.78980823, 0.79314505,\n",
       "        0.79400279, 0.79731869, 0.79230823, 0.79398884, 0.79898536,\n",
       "        0.79232566, 0.79816597, 0.79397838, 0.79481172, 0.79565202,\n",
       "        0.79065202, 0.79064505, 0.78982218, 0.78898187, 0.79231172,\n",
       "        0.78981172, 0.78814505, 0.79315202, 0.79148884, 0.79482915]),\n",
       " 'std_test_score': array([0.02866022, 0.02558792, 0.02803171, 0.03078384, 0.0215338 ,\n",
       "        0.02358406, 0.0286162 , 0.02624168, 0.03059148, 0.0200726 ,\n",
       "        0.02923181, 0.02969454, 0.02735578, 0.02497815, 0.02420624,\n",
       "        0.02230672, 0.024055  , 0.02383563, 0.02990863, 0.02767329,\n",
       "        0.03287901, 0.02960044, 0.0288634 , 0.02354197, 0.02624226,\n",
       "        0.02997893, 0.03551992, 0.03912403, 0.03293005, 0.03319746,\n",
       "        0.02875188, 0.03339763, 0.03692839, 0.03615927, 0.03817965,\n",
       "        0.03255176, 0.02642424, 0.04193765, 0.03677475, 0.03797705,\n",
       "        0.03361185, 0.04007255, 0.03408143, 0.03151471, 0.03610777,\n",
       "        0.03861176, 0.03497735, 0.03479989, 0.03272642, 0.0330437 ,\n",
       "        0.03428965, 0.02519032, 0.03347157, 0.03452855, 0.03370775,\n",
       "        0.03185727, 0.0352923 , 0.03442159, 0.0319461 , 0.03691636,\n",
       "        0.033485  , 0.03359644, 0.03887835, 0.03823401, 0.03308991,\n",
       "        0.02786507, 0.0343874 , 0.03256897, 0.0314192 , 0.03702189,\n",
       "        0.03242875, 0.03292916, 0.03239739, 0.034584  , 0.02974146]),\n",
       " 'rank_test_score': array([65, 65, 47, 54, 59, 46, 74, 72, 70, 69, 53, 34, 71, 68, 36, 67, 64,\n",
       "        26, 25, 75, 52, 35, 60, 56, 61, 16, 44, 51, 47, 58, 38, 23, 19, 39,\n",
       "         6,  8, 63,  9, 13, 32, 30, 57,  9, 27, 14, 30, 33, 50,  4, 28, 73,\n",
       "        62, 41, 44, 20, 12,  3, 23, 15,  1, 21,  2, 17, 11,  5, 37, 39, 42,\n",
       "        49, 22, 43, 55, 18, 28,  7])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridCV.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# fetch scores, reshape into a grid \n",
    "scores =gridCV.cv_results_['mean_test_score']\n",
    "scores = scores.reshape(len(max_features), len(n_estimators))\n",
    "#scores = np.transpose(scores)\n",
    "\n",
    "# Make heatmap from grid search results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(scores, interpolation='nearest', origin='higher', cmap='jet_r')\n",
    "plt.xticks(np.arange(len(n_estimators)), n_estimators)\n",
    "plt.yticks(np.arange(len(max_features)), max_features)\n",
    "plt.xlabel('Number of decision trees')\n",
    "plt.ylabel('Max features')\n",
    "plt.colorbar().set_label('Classification Accuracy', rotation=270, labelpad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can now train a new random forest on the wine dataset using what you have learned from the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       188\n",
      "         1.0       0.85      0.82      0.83       212\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.83      0.82       400\n",
      "weighted avg       0.83      0.82      0.83       400\n",
      "\n",
      "Overall Accuracy: 0.825\n"
     ]
    }
   ],
   "source": [
    "# Train classifier using optimal hyperparameter value\n",
    "# You could have also gotten this model out from gridCV.best_estimator_\n",
    "\n",
    "rf= RandomForestClassifier(n_estimators=best_n_estim,\n",
    "                          max_features=best_max_features)\n",
    "\n",
    "rf.fit(XTrain,yTrain)\n",
    "rf_preditcions = rf.predict(XTest)\n",
    "\n",
    "print(metrics.classification_report(yTest,rf_predictions))\n",
    "print(\"Overall Accuracy:\", round(metrics.accuracy_score(yTest,rf_predictions),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you tuned and applied your random forest classifier! You now get ~0.825 accuracy. This is quite a bit better than the poorly parameterized models which yielded ~0.815 accuracy. Importantly, it was not too difficult to get this boost – scikit can do grid search with k-fold CV in 1 line of code. How could you try to improve performance further?\n",
    "\n",
    "Note that grid search with k-fold CV simply returns the best HP values out of the available options, and is therefore not guaranteed to return a global optimum. It makes sense to choose a diverse collection of possible values that is somewhat centred around an empirically sensible default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "So, that was an overview of the concepts and practicalities involved when tuning a random forest classifer. You started with the motivation for tuning machine learning algorithms (i.e. nicer, bigger numbers in your models’ performance reports!). You then evaluated different candidate models by simple trial and error, as well as by using grid search with k-fold cross validation. You then ran the best possible model on the test set in order to predict wine quality from a set of chemical characteristics, and made correct predictions 81% of the time. You could have chosen to tune various other hyperpramaters, but the two referenced above are regarded as quite important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick quiz:\n",
    "Now that you’re familiar with adjusting and optimizing the behaviour of random forests in classification tasks, see if you can tackle these questions (feel free to leave your answers in the comments section):\n",
    "\n",
    "1. How do you think that altering the n_estimators and max_depth HPs would affect the bias and variance of a random forest classifier?\n",
    "\n",
    "n_estimators 越多可以減少overfitting(減少variance，增加bias),max_depth越大會造成overfitting(減少bias,增加variance)。\n",
    "\n",
    "2. To get more of an intuition of how random forests operate, play around with printing the importance of the features with print (rf.feature_importances_) under different conditions. What happens to the features importances when you set max_depth=0?\n",
    "\n",
    "不能把max_depth設成0。\n",
    "\n",
    "3. Try setting max_features=1. What does this force the trees in the random forest to do? `\n",
    "\n",
    "每次劃分就只找一個特徵就劃分了，不會比較其他特徵。\n",
    "\n",
    "4. Bonus question: It is interesting that the random forest performs better on the wine data with quite low values of max_features. What do you think this says about the features in the dataset?\n",
    "\n",
    "features leads overfitting,or theses features are high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07974502, 0.10265415, 0.07593293, 0.06637522, 0.08311479,\n",
       "       0.07066522, 0.09955795, 0.09514678, 0.07438876, 0.11034293,\n",
       "       0.14207625])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_features=1)\n",
    "rf.fit(XTrain,yTrain)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
